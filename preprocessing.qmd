---
title: "preprocessing"
format: html
editor: visual
---



# Install Packages

```{r}

install.packages("pacman")
library("pacman")

p_install("dplyr", force = FALSE)
p_install("ggplot2", force = FALSE)
p_install("readr", force = FALSE)
p_install("tidyr", force = FALSE)
p_install("sf", force = FALSE)
p_install("terra", force = FALSE)
p_install("tmap", force = FALSE)
p_install("zoo", force = FALSE)
p_install("units", force = FALSE)
p_install("plotly", force = FALSE)
p_install("patchwork", force = FALSE)
p_install("tidyverse", force = FALSE)
p_install("rjson", force = FALSE)
p_install("jsonlite", force = FALSE)
p_install("leaflet", force = TRUE)
p_install("shiny", force = TRUE)
p_install("XML", force = TRUE)
p_install("lubridate", force = TRUE)
p_install("ggh4x", force = TRUE)
p_install("forcats", force = TRUE)
p_install("purr", force = TRUE)
p_install("viridis", force = TRUE)
p_install("osrm", force = TRUE)
p_install("httr", force = TRUE)
p_install("mapmatchr", force = TRUE)

library("ggh4x")
library("dplyr")
library("ggplot2")
library("tidyr")
library("sf")
library("terra")
library("tmap")
library("zoo")
library("units")
library("plotly")
library("patchwork")
library("tidyverse")
library("rjson")
library("jsonlite")
library("leaflet")
library("XML")
library("lubridate")
library("shiny")
library("forcats")
library("purr")
library("viridis")
library("osrm")
library("httr")
library("mapmatchr")

```


# Task 1: Import Express: Bringing GPS Data Onboard

Folgend werden alle .gpx-Files aus dem Ordner *GPS_Daten* eingelesen. Aus den .gpx-Files werden die Koordinaten herausgelesen und jede gefahrene Route / Aktivität in einem neuen Dataframe abgespeichert. Die Dataframes landen alle in einer Liste `single_routes` und können über diese abgerufen und dargestellt werden.In einer weiteren Variate werden alle Daten in einen einzigen Data Frame verpackt `all_routes`.

```{r}


# generate a list of all filenames including the path from the subfolder they are stored in
file <- list.files("gps_files_shared", recursive = TRUE, pattern = "\\.gpx$", full.names = TRUE)

# Function to extract messenger and id from file path
extract_info <- function(file) {
  messenger <- as.factor(gsub(".*/gps_files_([^/]+)/.*", "\\1", file))
  id <- as.factor(paste(gsub('.*/(.*).gpx','\\1', file), gsub(".*/gps_files_([^/]+)/.*", "\\1", file), sep = "_"))
  list(messenger = messenger, id = id)
}

# Function to process each file
process_file <- function(file) {
  df <- st_read(file, "track_points")
  
  info <- extract_info(file)
  df$messenger <- info$messenger
  df$id <- info$id
  
  df_sf <- st_as_sf(df, coords = c("lon", "lat"), crs = 4326, remove = FALSE)
  df_sf <- st_transform(df_sf, crs = 2056)
  df_sf$shift <- gsub('.*/(.*).gpx','\\1', file)
  df_sf$x <- st_coordinates(df_sf)[,1]
  df_sf$y <- st_coordinates(df_sf)[,2]
  df_sf <- select(df_sf, id, messenger, shift, time, x, y, ele, geometry)
  
  df_sf$origin <- file
  df_sf
}

# Apply the function to each file using purrr's map function
single_routes <- purrr::map(file, process_file)

# Combine all results
all_routes <- do.call(rbind, single_routes)

raeubertochter_raw <- filter(all_routes, messenger == "raeubertochter")
donner_raw <- filter(all_routes, messenger == "donner")



# Quick overview
tmap_mode("plot")
tm_shape(single_routes[[2]]) +
  tm_dots( col = "messenger", palette = "seq", border.col = NULL)


```

# Task 2: Signal Sync: Assessing Sampling Intervals Across Messenger GPS Systems

```{r}

#If a larger sampling grid is needed: selecting every 10th row from  movement data
#all_routes <- all_routes[seq(from = 1, to = #nrow(all_routes), by = 5), ]

# calculate rowwise time difference
all_routes <- all_routes |> 
    group_by(id) |> 
    mutate(
    time_difference = as.numeric(difftime(time, lag(time), units = "secs"))) |>
    ungroup()


# How do the time difference differ between messengers?
all_routes |> 
  group_by(id) |> 
  filter(time_difference <= 30) |> # remove outliers to get a clearer view on the average sampling intervall
  summarise(
    mean <- mean(time_difference, na.rm = T)
    )

# max value between two fixes, in min
max(all_routes$time_difference, na.rm = T) / 60


ggplot(all_routes, aes(x = time_difference)) +
  geom_histogram(binwidth = 2) +
  scale_y_log10() +
  labs(x = "Time Difference in sec.", y = "Count (Log Scale)") +
  ggtitle("Histogram of Time Differences with Log Y Axis") +
  facet_wrap(all_routes$id) +
  theme_minimal()


# Both with similar but slightly different sampling regimes, raeubertochter with an intervall of 1 second- It seems that static time is already removed..

# Donner with more variation between fixes, but still short intervalls, might need another segmentation...

```

# Task 3: From Dots to Drops: Segmenting GPS Fixes into Deliveries

```{r}


# First we try the same segmentation method for all messenegrs

# create a moving time window
all_routes_seg <- all_routes |> 
   group_by(id) |> 
   mutate(
        nMinus2 = difftime(time, lag(time, 2)), 
        nMinus1 = difftime(time, lag(time,1)),  
        nPlus1  = difftime(lead(time, 1), time), 
        nPlus2  = difftime(lead(time, 2), time)  
    )


all_routes_seg <- all_routes_seg |> # calculate rowwise mean distance per messenger
    group_by(id) |> 
    rowwise() |>
    mutate(
        timeMean = mean(c(nMinus2, nMinus1, nPlus1, nPlus2))
    ) |>
    ungroup()


# create a new column static, based on time_difference (over 20s time difference)
all_routes_seg <- all_routes_seg |> 
  mutate(
    new_segment = timeMean > 20
  )

# it assigns unique IDs based on the column static
rle_id <- function(vec) {
    x <- rle(vec)$lengths
    as.factor(rep(seq_along(x), times = x))
}

# removes static rows
all_routes_seg <- all_routes_seg |>
    mutate(temp_id = rle_id(new_segment)) |> 
    filter(!new_segment)

# remove segments shorter than two minuntes
all_routes_seg <- all_routes_seg |> 
  group_by(temp_id) |> 
  mutate(duration = difftime(max(time), min(time))
  ) |> 
  filter(!duration < 120) |> 
  ungroup()

# Adssing new segment_id starting at one, credits to: https://stackoverflow.com/questions/39650511/r-group-by-variable-and-then-assign-a-unique-id
all_routes_seg <- all_routes_seg |> 
  group_by(temp_id, messenger) |> 
  mutate(segment_id = as_factor(cur_group_id())) |> 
  ungroup() |> 
  select(-temp_id)

# visualize segments
ggplot(all_routes_seg,aes(x, y, color = segment_id)) +
    scale_fill_brewer(palette = "Set3") +
    geom_point() +
    coord_fixed() +
    theme_minimal() +
    theme(legend.position = "none")+
    labs(title = "Deliveries", subtitle="Segmentation set at time_difference > 10s \nSegments < 60s removed") 


# It already looks really good????


all_routes_seg |> 
  group_by(messenger) |> 
  summarise(length(unique(segment_id)))


raeubertochter_seg <- filter(all_routes_seg, messenger == "raeubertochter")

donner_seg <- filter(all_routes_seg, messenger == "donner")


ggplot(donner_seg, aes(x, y, color = segment_id)) +
    geom_path() +
    geom_point() +
    scale_fill_brewer(palette = "Set1") +
    coord_fixed() +
    theme(legend.position = "none")+
    labs(title = "Deliveries", subtitle="Segmentation set at time_difference > 10s \nSegments < 120s removed")  

ggplot(raeubertochter_seg, aes(x, y, color = segment_id)) +
    geom_path() +
    geom_point() +
    scale_fill_brewer(palette = "Set1") +
    coord_fixed() +
    theme(legend.position = "none")+
    labs(title = "Deliveries", subtitle="Segmentation set at time_difference > 10s \nSegments < 120s removed")  


```

# Task 4: Lost in Transit: Tackling Tunnel GPS Blackouts for Bike Messenger

```{r}

all_routes_seg_tunnel <- all_routes_seg

first_last_points <- all_routes_seg_tunnel |> 
  group_by(segment_id) |> 
  slice(c(1, n())) |> 
  ungroup()

# Create a buffer around the tunnel entry and exit
tunnel_exit <- st_sfc(st_point(c(2682368, 1246996)), crs = 2056)
tunnel_entry <- st_sfc(st_point(c(2682592, 1246751)), crs = 2056)

# tunnel_points <- st_sfc(tunnel_entry, tunnel_exit, crs = 2056)
buffer_entry <- st_buffer(tunnel_entry, dist = 50)
buffer_exit <- st_buffer(tunnel_exit, dist = 50)


# Identify segments within the buffer
first_last_points_within <- first_last_points |> 
  mutate(
    intersect_start = sapply(st_intersects(geometry, buffer_entry, sparse = FALSE), any),
    intersect_end = sapply(st_intersects(geometry, buffer_exit, sparse = FALSE), any)
  ) |> 
  filter(intersect_start | intersect_end)

# Calculate time difference between end of one segment and start of the next
first_last_points_within <- first_last_points_within |> 
  arrange(segment_id, time) |> 
  mutate(time_diff = time - lag(time))

# Match segments where time difference is less than 2 minutes
matched_segments <- first_last_points_within |> 
  filter(time_diff <= 120 | is.na(time_diff) ) |> 
  mutate(new_segment_id = lag(segment_id)) |> 
  select(x,y,geometry,time_diff, segment_id, new_segment_id)

# create a table with matching segment starting / endpoints
lookup_table <- matched_segments |> 
  filter(!is.na(new_segment_id) & segment_id != new_segment_id) |> 
  select(segment_id, new_segment_id)  |> 
  distinct()

# Replace segment_id in all_routes_seg using lookup_vector, credits go to: https://stackoverflow.com/questions/35636315/replace-values-in-a-dataframe-based-on-lookup-table
lookup_vector <- setNames(lookup_table$new_segment_id, lookup_table$segment_id)

all_routes_seg_tunnel$segment_id_new <- ifelse(all_routes_seg_tunnel$segment_id %in% names(lookup_vector), 
                                    lookup_vector[match(all_routes_seg_tunnel$segment_id, names(lookup_vector))], 
                                    all_routes_seg_tunnel$segment_id)


all_routes_seg_tunnel_cor <- all_routes_seg_tunnel |> 
  group_by(segment_id_new, messenger) |> 
  mutate(segment_id_cor = as_factor(cur_group_id())) |> 
  select(-segment_id, -segment_id_new) |> 
  ungroup()


raeubertochter_cor <- filter(all_routes_seg_tunnel_cor, messenger == "raeubertochter")

donner_cor <- filter(all_routes_seg_tunnel_cor, messenger == "donner")


ggplot(donner_cor,aes(x, y, color = segment_id_cor)) +
    geom_path() +
    geom_point() +
    scale_fill_brewer(palette = "Set1") +
    coord_fixed() +
    theme(legend.position = "none")+
    labs(title = "Deliveries", subtitle="Segmentation set at time_difference > 10s \nSegments < 120s removed \nTunnel Challenge incl.")  

ggplot(raeubertochter_cor,aes(x, y, color = segment_id_cor)) +
    geom_path() +
    geom_point() +
    scale_fill_brewer(palette = "Set1") +
    coord_fixed() +
    theme(legend.position = "none")+
    labs(title = "Deliveries", subtitle="Segmentation set at time_difference > 10s \nSegments < 120s removed \nTunnel Challenge incl.")  
  

#################################################################################


## Interpolating!!! Should be done before merging the two objects. maybe it's better to interpolate based on the data frame matched_segments and then repeat the segmentation... cleaner version??


line <- st_cast(st_combine(matched_segments$geometry), "LINESTRING")

# Interpolate points along the line
interpolated_pts <- st_segmentize(line, units::set_units(5, "m"))

plot(interpolated_pts)
?st_cast

?st_segmentize


#################################################################################




```

# Task 4: Import Spatial Data

```{r}


st_layers("basic_data.gpkg")

streets <- read_sf("gis_files/basic_data.gpkg", "street_network") |> 
   select(objektart, geom) |> 
  mutate(
    objektart = as.factor(objektart),
    width = as.numeric(substr(objektart, start = 1, stop = 1)),
    width_scaled = as.numeric(substr(objektart, start = 1, stop = 1)) *1000
  ) |> 
  na.omit()
  


housing <- read_sf("gis_files/basic_data.gpkg", "housing_footprint") |> 
   select(objektart, geom) |> 
  mutate(
    objektart = as.factor(objektart)
  )


surface <- read_sf("gis_files/basic_data.gpkg", "surface_type") |> 
   select(art, geom) |> 
  mutate(
    art = as.factor(art)) |> 
 filter(art == "fliessendes Gewässer" | art == "stehendes Gewässer" | art == "Strasse, Weg" | art == "Verkehrsinsel") |> 
  na.omit()

summary(surface$art)


height <- terra::rast("gis_files/dhm25_zh.tif")

```



## Map Matching

```{r}

library(sf)
library(tmap)

################################### Nils #######################################################

# Define the coordinates of the polygon
coords <- rbind(c(2681695, 1247822), c(2683188, 1247822), c(2683188, 1246616), c(2681695, 1246616), c(2681695, 1247822))

# Create a polygon
polygon <- st_polygon(list(coords))

# Convert to spatial object
extent <- st_sfc(polygon, crs = 2056)


# since we want to find the closest location on the road over ALL roads
# we need to create a union of the roads first.
street_network <- st_union(streets)

?st_nearest_points

# Let's assume 'gps_locations' are your existing points
# gps_locations <- ...

# Now we can get the nearest point for each GPS location
nearest <- st_nearest_points(all_routes_seg_tunnel_cor, street_network)

# The output is a line for each point
# Now we need convert the output from LINE to POINT. 
# This doubles the number of features
near_p <- st_cast(nearest, "POINT")

# now we subset the points. Even numbers are the new, mapmatched points.
near_to <- near_p[c(FALSE,TRUE)]

# Update the geometry of the original points with the new locations
st_geometry(all_routes_seg_tunnel_cor) <- st_geometry(near_to)

tmap_mode("view")
tm_shape(street_network, bbox = extent) + 
  tm_lines() +
tm_shape(nearest) + 
  tm_lines(lty = 3) +
tm_shape(all_routes_seg_tunnel_cor$geometry) + 
  tm_dots(col = "red")




########################################### Valhalla ####################################################

system("docker pull valhalla/valhalla")

?jsonlite


map_matching <- function(df) {
  meili_coordinates <- jsonify::to_json(df)
  meili_head <- '{\"shape\":'
  meili_tail <- ",\"search_radius\": 50, \"shape_match\":\"map_snap\", \"costing\":\"auto\", \"format\":\"osrm\"}"
  meili_request_body <- paste(meili_head, meili_coordinates, meili_tail, sep = "")
  url <- "http://localhost:8002/trace_route"
  headers <- list('Content-type' = 'application/json')
  data <- str(meili_request_body)
  r <- httr::POST(url, body = data, headers = headers)
  if (r$status_code == 200) {
    response_text <- jsonlite::fromJSON(content(r, "text"), simplifyVector = FALSE)
    resp <- str(response_text$tracepoints)
    resp <- gsub("None", "{'matchings_index': '#', 'name': '', 'waypoint_index': '#', 'alternatives_count': 0, 'distance': 0, 'location': [0.0, 0.0]}", resp)
    resp <- gsub("'", "\"", resp)
    matched_df <- df.copy()
    df_response <- jsonlite::fromJSON(resp)
    for (i in seq_along(df_response)) {
      if (matched_df$lon[i] != 0.0) {
        matched_df$lon[i] <- df_response$location[[i]][1]
        matched_df$lat[i] <- df_response$location[[i]][2]
      }
    }
    return(matched_df)
  }
}

map_matching(all_routes_seg_tunnel_cor)



#################################### MAPMATCHR #######################################################

p_install("mapamtchr", force = TRUE)

```

# Enrich street network

```{r}

taj <- all_routes_seg_tunnel_cor |> 
  select(id, messenger, shift, time, geometry, duration, segment_id_cor)

tm_shape(all_routes_seg_tunnel_cor) +
  tm_dots()  +
  tm_shape(streets) +
  tm_lines()

taj2 <- st_buffer(taj, 0.25)

taj_width <- st_join(taj2, streets, left = TRUE, suffix = "street", join = st_intersects)


taj_count <- taj_width |> 
  st_drop_geometry() |> 
  group_by(messenger, segment_id_cor, width) |> 
  summarise(count = n()) |> 
  mutate(total = sum(count), 
         percentage = count / total * 100) |> 
ungroup()


head(taj_width)

taj_width <-  st_centroid(taj_width, crs = 2056)



plot(height)

extracted_height <- extract(height, taj_width)

taj_width$height <- round(extracted_height$dhm25_zh,2)

slope <- terrain(height, v = "slope", unit = "degrees", neighbors = 8)
extracted_slope <- extract(slope, taj_width)
taj_width$slope <- round(extracted_slope$slope,2)

taj_enriched <- taj_width |> 
  select(-width_scaled, -objektart)





taj_mahal <- taj_enriched %>%
  st_transform(crs = 2056) |> 
  arrange(segment_id_cor) %>%
  group_by(segment_id_cor) %>%
  mutate(height_diff = c(0, diff(height)),
         cumulative_up = cumsum(ifelse(height_diff > 0, height_diff, 0)),
         tot_up = sum(ifelse(height_diff > 0, height_diff, 0)),
         cumulative_down = cumsum(ifelse(height_diff < 0, height_diff, 0)),
         tot_down = sum(ifelse(height_diff < 0, height_diff, 0)),
         lead_geom = lead(geometry),
         travel_dist = sqrt(round(ifelse(!is.na(lead_geom), st_distance(lead_geom, geometry, by_element = TRUE), 0), 1))^2,
         slope_calc = atan(height_diff / travel_dist), 
         slope_up = cumsum(ifelse(slope_calc > 0, slope_calc, 0)),
         slope_tot_up = sum(ifelse(slope_calc > 0, slope_calc, 0)),
         slope_down = cumsum(ifelse(slope_calc < 0, slope_calc, 0)),
         slope_tot_down = sum(ifelse(slope_calc < 0, slope_calc, 0)),
         
         
         
         ) %>%
  ungroup()





subset_taj <- taj_enriched %>%
  filter(segment_id_cor %in% sample(unique(segment_id_cor), 5)) # Adjust the number of segments as needed

# Apply the same operations but print out the intermediate results
subset_taj <- subset_taj %>%
  st_transform(crs = 2056) %>%
  arrange(segment_id_cor) %>%
  group_by(segment_id_cor) %>%
  mutate(
    lag_geom = lag(geometry),
    travel_dist = st_distance(lag_geom, geometry)
  ) 

# Print out the results
head(subset_taj)




euclid <- function(x1, x2, y1, y2){
  distance <- sqrt((x2 - x1)^2 + (y2 - y1)^2)
  return(distance)
}




taj_mahal |> 
  filter(segment_id_cor == 1 | segment_id_cor == 2) |> 
 ggplot( aes(x = time, y = travel_dist, color = segment_id_cor)) +
  geom_line() +
  scale_fill_brewer(palette = "Set1")  # Use a color palette that suits your data

```

# Challenges

1. Loosing all information using spatial tools like st_cast, interpolating etc... we loose our information stored in different columns and only get back a point geometry...

2. Comparing time differences between pairs of last/first segment points in tunnel buffer...

3. Map matching only on roads that we care about

4. Do we work with points, line segments, nodes? How do we summarize for example slope on a given road? 